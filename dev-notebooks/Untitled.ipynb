{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livy_submit import livy_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = livy_api.LivyAPI(server_url='http://ip-172-31-20-241.ec2.internal:8998')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , offset, num, logs = api.log(81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchId = 88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, state = api.state(batchId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stdout: \n",
      "Warning: Ignoring non-spark config property: executorCores=4\n",
      "18/12/19 11:42:48 INFO SparkContext: Running Spark version 2.3.0\n",
      "18/12/19 11:42:48 INFO SparkContext: Submitted application: livy submit test\n",
      "18/12/19 11:42:48 INFO SecurityManager: Changing view acls to: livy,edill\n",
      "18/12/19 11:42:48 INFO SecurityManager: Changing modify acls to: livy,edill\n",
      "18/12/19 11:42:48 INFO SecurityManager: Changing view acls groups to: \n",
      "18/12/19 11:42:48 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/12/19 11:42:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, edill); groups with view permissions: Set(); users  with modify permissions: Set(livy, edill); groups with modify permissions: Set()\n",
      "18/12/19 11:42:48 INFO Utils: Successfully started service 'sparkDriver' on port 37343.\n",
      "18/12/19 11:42:48 INFO SparkEnv: Registering MapOutputTracker\n",
      "18/12/19 11:42:48 INFO SparkEnv: Registering BlockManagerMaster\n",
      "18/12/19 11:42:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "18/12/19 11:42:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "18/12/19 11:42:48 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-77b0bc6c-26b4-4926-9837-5c62872317d2\n",
      "18/12/19 11:42:48 INFO MemoryStore: MemoryStore started with capacity 2.2 GB\n",
      "18/12/19 11:42:48 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "18/12/19 11:42:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "18/12/19 11:42:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-172-31-20-241.ec2.internal:4040\n",
      "18/12/19 11:42:49 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "18/12/19 11:42:49 INFO RMProxy: Connecting to ResourceManager at ip-172-31-20-241.ec2.internal/172.31.20.241:8032\n",
      "18/12/19 11:42:49 INFO Client: Requesting a new application from cluster with 1 NodeManagers\n",
      "18/12/19 11:42:49 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (11520 MB per container)\n",
      "18/12/19 11:42:49 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\n",
      "18/12/19 11:42:49 INFO Client: Setting up container launch context for our AM\n",
      "18/12/19 11:42:49 INFO Client: Setting up the launch environment for our AM container\n",
      "18/12/19 11:42:49 INFO Client: Preparing resources for our AM container\n",
      "18/12/19 11:42:49 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-1514631244_24, ugi=edill (auth:PROXY) via livy/ip-172-31-20-241.ec2.internal@EMR.CONTIUUM.IO (auth:KERBEROS)]]\n",
      "18/12/19 11:42:49 INFO DFSClient: Created HDFS_DELEGATION_TOKEN token 580 for edill on 172.31.20.241:8020\n",
      "18/12/19 11:42:50 INFO metastore: Trying to connect to metastore with URI thrift://ip-172-31-20-241.ec2.internal:9083\n",
      "18/12/19 11:42:50 INFO metastore: Connected to metastore.\n",
      "18/12/19 11:42:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "18/12/19 11:42:54 INFO Client: Uploading resource file:/mnt/tmp/spark-51ca1ed7-25b9-45d6-bcd9-01f33ce7482c/__spark_libs__6188176883089138043.zip -> hdfs://ip-172-31-20-241.ec2.internal:8020/user/edill/.sparkStaging/application_1544723249474_0074/__spark_libs__6188176883089138043.zip\n",
      "18/12/19 11:42:56 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-172-31-20-241.ec2.internal:8020/user/edill/.sparkStaging/application_1544723249474_0074/hive-site.xml\n",
      "18/12/19 11:42:56 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-172-31-20-241.ec2.internal:8020/user/edill/.sparkStaging/application_1544723249474_0074/pyspark.zip\n",
      "18/12/19 11:42:56 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.6-src.zip -> hdfs://ip-172-31-20-241.ec2.internal:8020/user/edill/.sparkStaging/application_1544723249474_0074/py4j-0.10.6-src.zip\n",
      "18/12/19 11:42:56 INFO Client: Uploading resource file:/mnt/tmp/spark-51ca1ed7-25b9-45d6-bcd9-01f33ce7482c/__spark_conf__1155283313073398635.zip -> hdfs://ip-172-31-20-241.ec2.internal:8020/user/edill/.sparkStaging/application_1544723249474_0074/__spark_conf__.zip\n",
      "18/12/19 11:42:56 INFO SecurityManager: Changing view acls to: livy,edill\n",
      "18/12/19 11:42:56 INFO SecurityManager: Changing modify acls to: livy,edill\n",
      "18/12/19 11:42:56 INFO SecurityManager: Changing view acls groups to: \n",
      "18/12/19 11:42:56 INFO SecurityManager: Changing modify acls groups to: \n",
      "18/12/19 11:42:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy, edill); groups with view permissions: Set(); users  with modify permissions: Set(livy, edill); groups with modify permissions: Set()\n",
      "18/12/19 11:42:56 INFO Client: Submitting application application_1544723249474_0074 to ResourceManager\n",
      "18/12/19 11:42:57 INFO YarnClientImpl: Submitted application application_1544723249474_0074\n",
      "18/12/19 11:42:57 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1544723249474_0074 and attemptId None\n",
      "18/12/19 11:42:58 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:42:58 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: default\n",
      "\t start time: 1545219776993\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0074/\n",
      "\t user: edill\n",
      "18/12/19 11:42:59 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:00 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:01 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:02 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:03 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:04 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:05 INFO Client: Application report for application_1544723249474_0074 (state: ACCEPTED)\n",
      "18/12/19 11:43:05 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-172-31-20-241.ec2.internal, PROXY_URI_BASES -> http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0074), /proxy/application_1544723249474_0074\n",
      "18/12/19 11:43:05 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "18/12/19 11:43:05 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\n",
      "18/12/19 11:43:06 INFO Client: Application report for application_1544723249474_0074 (state: RUNNING)\n",
      "18/12/19 11:43:06 INFO Client: \n",
      "\t client token: Token { kind: YARN_CLIENT_TOKEN, service:  }\n",
      "\t diagnostics: N/A\n",
      "\t ApplicationMaster host: 172.31.28.18\n",
      "\t ApplicationMaster RPC port: 0\n",
      "\t queue: default\n",
      "\t start time: 1545219776993\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-172-31-20-241.ec2.internal:20888/proxy/application_1544723249474_0074/\n",
      "\t user: edill\n",
      "18/12/19 11:43:06 INFO YarnClientSchedulerBackend: Application application_1544723249474_0074 has started running.\n",
      "18/12/19 11:43:06 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34137.\n",
      "18/12/19 11:43:06 INFO NettyBlockTransferService: Server created on ip-172-31-20-241.ec2.internal:34137\n",
      "18/12/19 11:43:06 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "18/12/19 11:43:06 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 34137, None)\n",
      "18/12/19 11:43:06 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-20-241.ec2.internal:34137 with 2.2 GB RAM, BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 34137, None)\n",
      "18/12/19 11:43:06 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 34137, None)\n",
      "18/12/19 11:43:06 INFO BlockManager: external shuffle service port = 7337\n",
      "18/12/19 11:43:06 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-172-31-20-241.ec2.internal, 34137, None)\n",
      "18/12/19 11:43:06 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1544723249474_0074\n",
      "18/12/19 11:43:06 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances\n",
      "18/12/19 11:43:06 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8\n",
      "18/12/19 11:43:07 INFO SparkContext: Starting job: reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Got job 0 (reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47) with 2 output partitions\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Final stage: ResultStage 0 (reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47)\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Parents of final stage: List()\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Missing parents: List()\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Submitting ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47), which has no missing parents\n",
      "18/12/19 11:43:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.5 KB, free 2.2 GB)\n",
      "18/12/19 11:43:07 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 2.2 GB)\n",
      "18/12/19 11:43:07 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-20-241.ec2.internal:34137 (size: 3.0 KB, free: 2.2 GB)\n",
      "18/12/19 11:43:07 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1079\n",
      "18/12/19 11:43:07 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (PythonRDD[1] at reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47) (first 15 tasks are for partitions Vector(0, 1))\n",
      "18/12/19 11:43:07 INFO YarnScheduler: Adding task set 0.0 with 2 tasks\n",
      "18/12/19 11:43:08 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 1)\n",
      "18/12/19 11:43:09 INFO ExecutorAllocationManager: Requesting 1 new executor because tasks are backlogged (new desired total will be 2)\n",
      "18/12/19 11:43:13 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.18:40160) with ID 1\n",
      "18/12/19 11:43:13 INFO ExecutorAllocationManager: New executor 1 has registered (new total is 1)\n",
      "18/12/19 11:43:13 WARN TaskSetManager: Stage 0 contains a task of very large size (371 KB). The maximum recommended task size is 100 KB.\n",
      "18/12/19 11:43:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, ip-172-31-28-18.ec2.internal, executor 1, partition 0, PROCESS_LOCAL, 380245 bytes)\n",
      "18/12/19 11:43:13 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-18.ec2.internal:34781 with 1028.8 MB RAM, BlockManagerId(1, ip-172-31-28-18.ec2.internal, 34781, None)\n",
      "18/12/19 11:43:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-28-18.ec2.internal:34781 (size: 3.0 KB, free: 1028.8 MB)\n",
      "18/12/19 11:43:13 INFO YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.31.28.18:40164) with ID 2\n",
      "18/12/19 11:43:13 INFO ExecutorAllocationManager: New executor 2 has registered (new total is 2)\n",
      "18/12/19 11:43:13 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, ip-172-31-28-18.ec2.internal, executor 2, partition 1, PROCESS_LOCAL, 508048 bytes)\n",
      "18/12/19 11:43:14 INFO BlockManagerMasterEndpoint: Registering block manager ip-172-31-28-18.ec2.internal:38863 with 1028.8 MB RAM, BlockManagerId(2, ip-172-31-28-18.ec2.internal, 38863, None)\n",
      "18/12/19 11:43:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-172-31-28-18.ec2.internal:38863 (size: 3.0 KB, free: 1028.8 MB)\n",
      "18/12/19 11:43:15 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1935 ms on ip-172-31-28-18.ec2.internal (executor 1) (1/2)\n",
      "18/12/19 11:43:15 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 1595 ms on ip-172-31-28-18.ec2.internal (executor 2) (2/2)\n",
      "18/12/19 11:43:15 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "18/12/19 11:43:15 INFO DAGScheduler: ResultStage 0 (reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47) finished in 8.365 s\n",
      "18/12/19 11:43:15 INFO DAGScheduler: Job 0 finished: reduce at /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06/pi.py:47, took 8.440610 s\n",
      "Pi is roughly 3.143720\n",
      "18/12/19 11:43:15 INFO SparkUI: Stopped Spark web UI at http://ip-172-31-20-241.ec2.internal:4040\n",
      "18/12/19 11:43:15 INFO YarnClientSchedulerBackend: Interrupting monitor thread\n",
      "18/12/19 11:43:15 INFO YarnClientSchedulerBackend: Shutting down all executors\n",
      "18/12/19 11:43:15 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\n",
      "18/12/19 11:43:15 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices\n",
      "(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\n",
      "18/12/19 11:43:15 INFO YarnClientSchedulerBackend: Stopped\n",
      "18/12/19 11:43:15 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
      "18/12/19 11:43:15 INFO MemoryStore: MemoryStore cleared\n",
      "18/12/19 11:43:15 INFO BlockManager: BlockManager stopped\n",
      "18/12/19 11:43:15 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
      "18/12/19 11:43:15 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
      "18/12/19 11:43:15 INFO SparkContext: Successfully stopped SparkContext\n",
      "18/12/19 11:43:16 INFO ShutdownHookManager: Shutdown hook called\n",
      "18/12/19 11:43:16 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-e2896c30-410b-4e57-ad39-fd9ab6ccfc06\n",
      "18/12/19 11:43:16 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-51ca1ed7-25b9-45d6-bcd9-01f33ce7482c\n",
      "18/12/19 11:43:16 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-51ca1ed7-25b9-45d6-bcd9-01f33ce7482c/pyspark-e769ca60-4e6b-447a-a331-4517bebe6ef3\n"
     ]
    }
   ],
   "source": [
    "prev_num = 0\n",
    "prev_offset = 0\n",
    "while state in ('running', 'starting'):\n",
    "    _, state = api.state(batchId)\n",
    "    _, offset, num, logs = api.log(batchId)\n",
    "    \n",
    "    # Split the logs into stdout/stderr\n",
    "    stderr = logs.index('\\nstderr: ')\n",
    "    stdout_logs = logs[:stderr]\n",
    "    stderr_logs = logs[stderr:]\n",
    "    \n",
    "    # How many new lines are there?\n",
    "    new_lines = num - prev_num\n",
    "    if new_lines > 0:\n",
    "        logger.info('\\n'.join(stdout_logs[-new_lines:]))\n",
    "    \n",
    "    # Store the state for the next loop iteration\n",
    "    prev_num = num\n",
    "    prev_offset = offset\n",
    "    # Delay a little bit so we're not hammering the Livy server\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iiii"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "livy-submit [py3]",
   "language": "python",
   "name": "livy-submit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
