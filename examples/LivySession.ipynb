{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LivySession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='livy.png' width='60%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livy REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Endpoints:\n",
    "* `/batches`: To submit batch files that are already on HDFS.\n",
    "* `/sessions`: Interactive exeuction.\n",
    "    * `/sessions/<id>/statements`: Used by Sparkmagic. Sends strings of code.\n",
    "    * `/sessions/<id>/jobs`: Undocumeted endpoint to submit pickled functions interactively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rebuilt Python API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Light-weight Python module called `LivySession`. Currently a PR to Eric's `AnacondaPlatform/livy-submit`.\n",
    "\n",
    "Supports both `statements` and `jobs`. Works in both scripts and Notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from livy_submit import kinit_username, LivySession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticket cache: FILE:/tmp/krb5cc_501\n",
      "Default principal: instructor@TRAINING.ANACONDA.COM\n",
      "\n",
      "Valid starting     Expires            Service principal\n",
      "07/19/19 10:57:46  07/19/19 20:57:46  krbtgt/TRAINING.ANACONDA.COM@TRAINING.ANACONDA.COM\n",
      "\trenew until 07/20/19 10:57:46\n",
      "\n",
      "Starting session 153\n"
     ]
    }
   ],
   "source": [
    "kinit_username('instructor', 'anaconda')\n",
    "\n",
    "pypath = '/opt/anaconda3/bin/python'\n",
    "config =  {\"spark.yarn.appMasterEnv.PYSPARK_PYTHON\": pypath, \n",
    "           \"spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON\":  pypath,\n",
    "           \"spark.yarn.executorEnv.PYSPARK_PYTHON\": pypath,\n",
    "           \"spark.pyspark.python\": pypath,\n",
    "           \"spark.pyspark.driver.python\": pypath}\n",
    "\n",
    "\n",
    "host = 'http://livy.training.anaconda.com:8998'\n",
    "pyspark = LivySession(host, conf = {\"conf\": config})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awkward to prepare code as string. Sparkmagic does the hard work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for http://livy.training.anaconda.com:8998/sessions/153\n",
      "Waiting for http://livy.training.anaconda.com:8998/sessions/153/statements/0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 0,\n",
       " 'code': \"x = [\\n    {'a': 1, 'b': 2},\\n    2/3\\n]\\n%json x\",\n",
       " 'state': 'available',\n",
       " 'output': {'status': 'ok',\n",
       "  'execution_count': 0,\n",
       "  'data': {'application/json': [{'a': 1, 'b': 2}, 0.6666666666666666]}},\n",
       " 'progress': 1.0}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code = '''x = [\n",
    "    {'a': 1, 'b': 2},\n",
    "    2/3\n",
    "]\n",
    "%json x'''\n",
    "\n",
    "x = pyspark.execute(code)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': 2}, 0.6666666666666666]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['output']['data']['application/json']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decorator does the following on run:\n",
    "1. cloudpickle the function with supplied arguemnts (including functions it references)\n",
    "1. submit job and wait\n",
    "     * Spark pickles all returned values\n",
    "1. unpickle values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for http://livy.training.anaconda.com:8998/sessions/153\n",
      "Waiting for http://livy.training.anaconda.com:8998/sessions/153/jobs/1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origin</th>\n",
       "      <th>avg(mpg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Europe</td>\n",
       "      <td>27.602941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>America</td>\n",
       "      <td>20.033469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Asia</td>\n",
       "      <td>30.450633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    origin   avg(mpg)\n",
       "0   Europe  27.602941\n",
       "1  America  20.033469\n",
       "2     Asia  30.450633"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@pyspark\n",
    "def grouper(hive_table, group_col, value_col, metric):\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.enableHiveSupport().getOrCreate()\n",
    "    \n",
    "    df = spark.table(hive_table)\n",
    "    grp = df.groupby(group_col)\n",
    "    result = getattr(grp, metric)(value_col)\n",
    "    \n",
    "    return result.toPandas()\n",
    "\n",
    "mpg = grouper('autompg', 'origin', 'mpg', 'mean')\n",
    "mpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for http://livy.training.anaconda.com:8998/sessions/153\n",
      "stopped http://livy.training.anaconda.com:8998/sessions/153\n"
     ]
    }
   ],
   "source": [
    "pyspark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it can be pickled by cloudpickle it can be sent both ways.\n",
    "\n",
    "Packages and versions should match between local environment and Spark session.\n",
    "* `conda_pack` --> S3, HDFS --> `spark.pyspark.python`\n",
    "* Ask Eric for more details.\n",
    "\n",
    "TODO: Python files can be sent before the function is called to allow imports of non-packaged code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting session 154\n",
      "Waiting for http://livy.training.anaconda.com:8998/sessions/154\n",
      "Waiting for http://livy.training.anaconda.com:8998/sessions/154/jobs/1\n",
      "Waiting for http://livy.training.anaconda.com:8998/sessions/154\n",
      "stopped http://livy.training.anaconda.com:8998/sessions/154\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pi_wallis(n):                                                          \n",
    "    x = np.arange(1,n)                                                          \n",
    "    t = 4*x*x                                                                   \n",
    "    y = t / (t - 1)                                                             \n",
    "    z = 2.0 * y.prod()                                                          \n",
    "    return z\n",
    "\n",
    "\n",
    "with LivySession(host, conf={'conf':config}) as new_session:\n",
    "    @new_session\n",
    "    def pi(n):\n",
    "        import numpy as np\n",
    "        from pyspark.context import SparkContext\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        from random import random\n",
    "        \n",
    "        def sample(_):\n",
    "            '''Find random numbers within the unit circle'''\n",
    "            x = random()\n",
    "            y = random()\n",
    "            return 1 if x*x + y*y < 1 else 0\n",
    "        \n",
    "        count = (sc\n",
    "                 .parallelize(range(0, n))\n",
    "                 .map(sample)\n",
    "                 .reduce(lambda a, b: a + b)\n",
    "                )\n",
    "        \n",
    "        pi_spark = count * 4 / n\n",
    "        pi_numpy = pi_wallis(n)\n",
    "        \n",
    "        return np.array([pi_spark, pi_numpy])\n",
    "\n",
    "    result = pi(int(1e4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.14      , 3.14151411])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Does this seem at all useful compared to Sparkmagic + livy_submit?\n",
    "* What would you like to see?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:livy]",
   "language": "python",
   "name": "conda-env-livy-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
