
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Use livy-submit as a Python Library &#8212; livy-submit 0.5.4 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="livy-submit as REST API" href="as_rest_api.html" />
    <link rel="prev" title="Use livy-submit as a CLI tool" href="cli_docs.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="use-livy-submit-as-a-python-library">
<h1>Use livy-submit as a Python Library<a class="headerlink" href="#use-livy-submit-as-a-python-library" title="Permalink to this headline">¶</a></h1>
<p>livy-submit is both a CLI tool and a Python library.
The CLI piece makes use of the Python API so all functionality that is exposed by the CLI is also available inside of a Jupyter notebook or a Python file (or another Python library if you wish to build on top of livy submit).
There are three modules inside of livy submit that you’ll want to be aware of if you’re going to be importing livy submit into your Jupyter notebook or Python file:</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">livy_submit/livy_api.py</span></code>: This is a Python API around all of the Livy REST API /batches endpoints.
There are a number of endpoints on Livy to manage batch jobs and all are made available via the livy_api.py module inside of livy_submit.
There is a LivyAPI class inside of this module.
Use this class for full access to the /batches operations on the Livy REST API.
See below for more detail.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">livy_submit/hdfs_api.py</span></code>: This is a wrapper around another Python HDFS library that exposes some useful functionality:
a. An upload function that can upload a local file to an hdfs directory
b. A delete function that can be used to remove an hdfs directory
c. A get_client function that can be used to get the underlying Python HDFS client for full HDFS file system access</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">livy_submit/krb.py</span></code>: This is where you’d go for two kinit functions.
a. <code class="docutils literal notranslate"><span class="pre">kinit_keytab</span></code>: Use a keytab to create a Kerberos TGT for your session
b. <code class="docutils literal notranslate"><span class="pre">kinit_username</span></code>: Use a username and password to create a Kerberos TGT for your session</p></li>
</ol>
<div class="section" id="livyapi">
<h2>LivyAPI<a class="headerlink" href="#livyapi" title="Permalink to this headline">¶</a></h2>
<p>To use the LivyAPI directly, you’ll need to know the url+port for your Livy server.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">livy_submit</span> <span class="kn">import</span> <span class="n">LivyAPI</span>
<span class="n">server_url</span> <span class="o">=</span> <span class="s1">&#39;http://ip-172-31-20-241.ec2.internal:8998&#39;</span>
 
<span class="n">livy_api</span> <span class="o">=</span> <span class="n">LivyAPI</span><span class="p">(</span><span class="n">server_url</span><span class="o">=</span><span class="n">server_url</span><span class="p">)</span>
</pre></div>
</div>
<p>Now that we have the livy_api, we can interact with it.
With the following six functions you can get quite sophisticated in terms of what you can do with submitting spark job, monitoring their progress and checking their final state.
I suspect it would not be too difficult to build your own basic pipeline based on this API and batch job submission.</p>
<ol class="simple">
<li><p>I can get the info of all available sessions via the <code class="docutils literal notranslate"><span class="pre">livy_api.all_info()</span></code> function.
This will return a list of Batch namedtuples that have the following fields (in order):
a. <code class="docutils literal notranslate"><span class="pre">id</span></code>: The Livy batch ID
b. <code class="docutils literal notranslate"><span class="pre">appId</span></code>: The Yarn application ID, if it has been set yet
c. <code class="docutils literal notranslate"><span class="pre">appInfo</span></code>: Dictionary that contains the driver log URL and the spark UI URL
d. <code class="docutils literal notranslate"><span class="pre">log</span></code>: Up to the last 100 log lines from the Yarn application
e. <code class="docutils literal notranslate"><span class="pre">state</span></code>: The current state of the Batch job. Should be one of “starting”, “running, “finished” or “error” (though I may be wrong on exactly what these options are. They’re not documented by Livy anywhere).</p></li>
<li><p>I can get the info of a specific session via the <code class="docutils literal notranslate"><span class="pre">livy_api.info(batch_id)</span></code> function. This returns one Batch object or raises a 403 HTTP error</p></li>
<li><p>I can get the state of a specific session via the <code class="docutils literal notranslate"><span class="pre">livy_api.state(batch_id)</span></code> function. This returns a tuple of (batch_id, state)</p></li>
<li><p>I can get the most recent log lines via the <code class="docutils literal notranslate"><span class="pre">livy_api.log(batch_id)</span></code> function.</p></li>
<li><p>I can kill the job via the <code class="docutils literal notranslate"><span class="pre">livy_api.kill(batch_id)</span></code> function.</p></li>
<li><p>I can submit a new job to be executed via the <code class="docutils literal notranslate"><span class="pre">livy_api.submit()</span></code> function. This submit function takes a lot of parameters:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Parameters
----------
name : str
    The name that your Spark job should have on the Yarn RM
file : str
    The file that should be executed during your Spark job. This file
    path must be accessible from the nodes that run your Spark driver/executors.
    It is likely that this means that you will need to have pre-uploaded your file
    to hdfs and then use the hdfs path for this `file` variable.
    e.g.: file=&#39;hdfs://user/testuser/pi.py&#39;
driverMemory : str, optional
    e.g. 512m, 2g
    Amount of memory to use for the driver process, i.e. where
    SparkContext is initialized, in the same format as JVM memory
    strings with a size unit suffix (&quot;k&quot;, &quot;m&quot;, &quot;g&quot; or &quot;t&quot;)
driverCores : int, optional
        Number of cores to use for the driver process, only in cluster mode.
executorMemory : str, optional
    e.g. 512m, 2g
    Amount of memory to use per executor process, in the same format as
    JVM memory strings with a size unit suffix (&quot;k&quot;, &quot;m&quot;, &quot;g&quot; or &quot;t&quot;)
executorCores : int, optional
    The number of cores to use on each executor
archives : List of strings
    Archives to be used in this session. Same deal as the `file` parameter. These
    archives likely need to already be uploaded to HDFS unless the files already
    exist on your Yarn nodes or if you have something like NFS available on all
    of the Yarn nodes.
queue : str
    The YARN queue that your job should run in
conf : dict
    Additonal spark configuration properties. Any valid variable listed in the
    spark configuration for your version of spark. See all here
    https://spark.apache.org/docs/latest/configuration.html
    e.g. {&#39;spark.pyspark.python&#39;: &#39;/opt/anaconda3/bin/python&#39;}
args : list of strings
    Extra command line args for the application. If your python main is expecting
    command line args, use this variable to pass them in.
pyFiles: list of strings
    Python files to be used in this session. Same deal as the `file` parameter.
    These archives need to be already uploaded to HDFS.
</pre></div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">livy-submit</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="as_cli.html">Using livy-submit from the command line</a></li>
<li class="toctree-l1"><a class="reference internal" href="cli_docs.html">livy-submit CLI docs</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Using livy-submit as a library</a></li>
<li class="toctree-l1"><a class="reference internal" href="as_rest_api.html">Using livy-submit inside a REST API</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../livy_submit.html">API docs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">module list</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="cli_docs.html" title="previous chapter">Use livy-submit as a CLI tool</a></li>
      <li>Next: <a href="as_rest_api.html" title="next chapter">livy-submit as REST API</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Anaconda.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/examples/as_library.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>